{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid19 - Twitter data extraction\n",
    "by Victoria, Maha, Gopi\n",
    "\n",
    "## Table of contents\n",
    "- Introduction\n",
    "- Authenticatications\n",
    "    - Twitter\n",
    "    - Google sheets\n",
    "- Gathering data & storing\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This notebook is part of the project developed for the FLT Big Data Hackathon, whose objective is to create interesting and trustworthy analyses and visualizations about the COVID19 situation and its correlation with the stock market. \n",
    "\n",
    "In this notebook we use the Twitter API to retrieve the tweets related to COVID19 hashtags and economic tags, to perform a sentimental analysis and store it programatically in a google sheets file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\v.perez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load important libraries\n",
    "import gspread \n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import json\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from tweepy import Stream\n",
    "from tweepy import StreamListener\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "from  geopy.geocoders import Nominatim\n",
    "from datetime import datetime\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "### Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "jqdQzdqytO88",
    "outputId": "b46f1541-50e8-4fe4-a091-00175482cd65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n",
      "printing tweets from timeline \n",
      " \n",
      "Che, si esto de comprar en Amazon ES (https://t.co/ZKbkWkTHBD) y recibir en casa directo (sin tener que ir a busca… https://t.co/vEP9QrhZw5\n",
      "\n",
      "RT @mariana_aran: Se puede, se sigue, se sana, se recompone, se vuelve a tener fe, se pierde, se gana, se extravía, se encuentra, se pide y…\n",
      "\n",
      "#Cuarentena Apertura de comercios: la UCIP insiste en el pedido, con otra carta a la Provincia\n",
      "https://t.co/W6v0utj9Yu\n",
      "\n",
      "#Seguridad Le robaron a una mujer de 88 años en su casa del barrio Don Bosco https://t.co/g7G78NDWps\n",
      "\n",
      "RT @jimenaplopez: Los derechos de las personas gestantes y recién nacides no entraron en cuarentena #SMPR2020 https://t.co/i3yutwJ7Yv por @…\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load twitter credentials\n",
    "with open(\"covid19-sentanalysis-twitter_credentials.json\") as datafile:\n",
    "  data = json.load(datafile)\n",
    "\n",
    "# Define the keys\n",
    "consumer_key= data['consumer_key'] #'API_CONSUMER_KEY_HERE'\n",
    "consumer_secret=  data['consumer_secret']#'CONSUMER_SECRET_HERE'\n",
    "\n",
    "access_token= data['access_token_key'] #'ACCESS_TOKEN_HERE'\n",
    "access_token_secret= data['access_token_secret'] #'ACCESS_TOKEN_SECRET_HERE'\n",
    "\n",
    "\n",
    "#Crate the auth object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# create API, set limits to avouid errors because of a timeout \n",
    "api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")\n",
    "\n",
    "#Print 5 tweets for testing purposes - Should be deleted afterwards\n",
    "home_tweets = api.home_timeline(count=5)\n",
    "print(\"printing tweets from timeline \\n \")\n",
    "for tweet in home_tweets:\n",
    "    print(tweet.text)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = [\n",
    "   'https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "#authenticate gsheets\n",
    "google_key_file = 'service_key.json'\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(google_key_file, scope)\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "# Define spreadsheet access\n",
    "spreadsheet_key = '1auoQ9XanosnM7RUInzqeZi9EIgwtCtmtubNpXrfF6OM' \n",
    "wks_name = 'sentimentAnalysis'\n",
    "\n",
    "# Open the file\n",
    "book = gc.open_by_key(spreadsheet_key) \n",
    "worksheet = book.worksheet(wks_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlNcN1cx0uOD"
   },
   "source": [
    "## Gathering data & storing\n",
    "**GET Twitter Stream and Do Sentiment Analysis in Real time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "huknvBk9Y8bj"
   },
   "outputs": [],
   "source": [
    "trump = 0\n",
    "warren = 0\n",
    "\n",
    "header_name = ['id', 'user_id','Text','created_at','timestamp','location','latitude','longitude','followers_count','Trump','Warren']\n",
    "\n",
    "class Listener(StreamListener):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_tweets = 10\n",
    "        self.tweet_count = 0\n",
    "        self.geolocator = Nominatim()\n",
    "        self.tweet_list = []\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        raw_twitts = json.loads(data)\n",
    "        try:\n",
    "            #  Fields we need: id, created_at, text, coordinates, author_id\n",
    "            full_tweets = raw_twitts.copy()\n",
    "            # TO DO: we must drop from full_tweets the tweets that consist only on RT, numbers, etc (see regex used below)\n",
    "            tweets = raw_twitts['text']\n",
    "            tweets = ' '.join(re.sub(\"(@[A-Za-z0-9]+) | ({*0-9A-Za-z \\t]) |] (\\wt:\\/\\/\\St+)\", \" \", tweets).split())\n",
    "            tweets = ' '.join(re.sub('RT',' ', tweets).split())  \n",
    "  \n",
    "  \n",
    "            blob = TextBlob(tweets)\n",
    "            print(\"start blob:\")\n",
    "            print(blob)\n",
    "            print(\"finish blob:\")\n",
    "            global trump\n",
    "            global warren\n",
    "  \n",
    "            trump_sentiment = 0\n",
    "            warren_sentiment = 0\n",
    "  \n",
    "            for sent in blob.sentences:\n",
    "                if \"Trump\" in sent and \"Warren\" not in sent:\n",
    "                    trump_sentiment = trump_sentiment + sent.sentiment.polarity\n",
    "                else:\n",
    "                    warren_sentiment = warren_sentiment + sent.sentiment.polarity\n",
    "    \n",
    "            trump = trump + trump_sentiment\n",
    "            warren = warren + warren_sentiment\n",
    "  \n",
    "            #get timestamp from created_at\n",
    "            time_created_at = raw_twitts['created_at']\n",
    "            t = time_created_at.split('+0000 ')\n",
    "            time = t[0] +t[1]\n",
    "            format_time = '%a %b %d %H:%M:%S %Y'\n",
    "            date_time = datetime.strptime(time,format_time)\n",
    "            ts = int(date_time.timestamp())\n",
    "            \n",
    "            #get lat, long from location\n",
    "            lat = None\n",
    "            long = None\n",
    "            if raw_twitts['user']['location']:\n",
    "                loc = self.geolocator.geocode(raw_twitts['user']['location'])\n",
    "                if loc:\n",
    "                    lat = loc.latitude\n",
    "                    long = loc.longitude\n",
    "            \n",
    "            if lat and long:\n",
    "                info = {'id':raw_twitts['id'],\n",
    "                            'user_id':raw_twitts['user']['id'], \n",
    "                            'Text':raw_twitts['text'],\n",
    "                            'created_at':raw_twitts['created_at'],\n",
    "                            'timestamp':ts,\n",
    "                            'location':raw_twitts['user']['location'],\n",
    "                            'latitude':lat,\n",
    "                            'longitude':long,\n",
    "                            'followers_count':raw_twitts['user']['followers_count'],\n",
    "                            'Trump': trump,\n",
    "                            'Warren': warren}\n",
    "\n",
    "                self.tweet_list.append(info)\n",
    "  \n",
    "            print (tweets,'\\n')    \n",
    "        except:\n",
    "            print('ERROR got')\n",
    "        else:\n",
    "            self.tweet_count+=1\n",
    "                # Once it reaches a fix limit the Write the data into gsheets\n",
    "            if(self.tweet_count==self.max_tweets):          \n",
    "                # save to a dataframe for eeasier file upload\n",
    "                  df_tweet_list = pd.DataFrame(self.tweet_list, columns = header_name)\n",
    "            \n",
    "                  d2g.upload(df_tweet_list, spreadsheet_key, wks_name,clean =False, credentials=credentials, row_names=False)\n",
    "            \n",
    "                  print(\"completed\")\n",
    "                  return(False)\n",
    "            else:\n",
    "                decoded = json.loads(data)\n",
    "\n",
    "        def on_error(self, status):\n",
    "            print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u-MAEzOxY9k9"
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jE5iBNqMY-D5",
    "outputId": "192cc8f0-3c22-4cd0-e3c2-ea65a512c153"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\v.perez\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Using Nominatim with the default \"geopy/1.22.0\" `user_agent` is strongly discouraged, as it violates Nominatim's ToS https://operations.osmfoundation.org/policies/nominatim/ and may possibly cause 403 and 429 HTTP errors. Please specify a custom `user_agent` with `Nominatim(user_agent=\"my-application\")` or by overriding the default `user_agent`: `geopy.geocoders.options.default_user_agent = \"my-application\"`. In geopy 2.0 this will become an exception.\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start blob:\n",
      "@tripp_gordon: Christian... know about the authors on your shelf.\n",
      "finish blob:\n",
      "@tripp_gordon: Christian... know about the authors on your shelf. \n",
      "\n",
      "start blob:\n",
      "@cali_jetfan: The rank hypocrisy of GOP Sen. Lindsey Graham when it comes to Donald Trump is laid bare in a searing new attack ad. https…\n",
      "finish blob:\n",
      "@cali_jetfan: The rank hypocrisy of GOP Sen. Lindsey Graham when it comes to Donald Trump is laid bare in a searing new attack ad. https… \n",
      "\n",
      "start blob:\n",
      "@RoscoeJames: Was it the CDC that predicted 100,00 deaths by the end of this month? I'm guessing 110,000. And all because the Sena…\n",
      "finish blob:\n",
      "@RoscoeJames: Was it the CDC that predicted 100,00 deaths by the end of this month? I'm guessing 110,000. And all because the Sena… \n",
      "\n",
      "start blob:\n",
      "@ofmkharkivukr: THANKS TO ALMIGHTY GOD FOR A PRESIDENT LIKE TRUMP. https://t.co/dQG5je248D\n",
      "finish blob:\n",
      "@ofmkharkivukr: THANKS TO ALMIGHTY GOD FOR A PRESIDENT LIKE TRUMP. https://t.co/dQG5je248D \n",
      "\n",
      "start blob:\n",
      "@C_3C_3: Cuomo cried about ventilators until it was discovered he’d hidden 30K, he needed the USNS Comfort/field hospitals built that he…\n",
      "finish blob:\n",
      "@C_3C_3: Cuomo cried about ventilators until it was discovered he’d hidden 30K, he needed the USNS Comfort/field hospitals built that he… \n",
      "\n",
      "start blob:\n",
      "@oh_melodylane: Let's help with her little poll 😅\n",
      "finish blob:\n",
      "@oh_melodylane: Let's help with her little poll 😅 \n",
      "\n",
      "start blob:\n",
      "@JoeBiden: Think of how many small businesses could have been helped with the $47 million that went to private jet companies owned by we…\n",
      "finish blob:\n",
      "@JoeBiden: Think of how many small businesses could have been helped with the $47 million that went to private jet companies owned by we… \n",
      "\n",
      "start blob:\n",
      "@realDonaldTrump: Sleepy Joe cannot bring us to greatness. He is the reason I’m here!\n",
      "finish blob:\n",
      "@realDonaldTrump: Sleepy Joe cannot bring us to greatness. He is the reason I’m here! \n",
      "\n",
      "start blob:\n",
      "@funder: A GOP chair in Texas says coronavirus is a hoax made up by Democrats, she then asks everyone to take off their masks and they a…\n",
      "finish blob:\n",
      "@funder: A GOP chair in Texas says coronavirus is a hoax made up by Democrats, she then asks everyone to take off their masks and they a… \n",
      "\n",
      "start blob:\n",
      "#Global : Democrats unveil two articles of impeachment against President Donald Trump : Reacting to the developmen… https://t.co/WwfANYF5tM\n",
      "finish blob:\n",
      "#Global : Democrats unveil two articles of impeachment against President Donald Trump : Reacting to the developmen… https://t.co/WwfANYF5tM \n",
      "\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "twitter_stream = Stream(auth, Listener())\n",
    "twitter_stream.filter(track = ['Trump','Warren'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HTHQoydfkCz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "twitter_data_extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
