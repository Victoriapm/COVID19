{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7KH3AiAns0od"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load gsheets libraries\n",
    "import gspread \n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "\n",
    "scope = [\n",
    "   'https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "#authenticate gsheets\n",
    "google_key_file = 'service_key.json'\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(google_key_file, scope)\n",
    "gc = gspread.authorize(credentials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spreadsheet_key = '1auoQ9XanosnM7RUInzqeZi9EIgwtCtmtubNpXrfF6OM' \n",
    "wks_name = 'sentimentAnalysis'\n",
    "\n",
    "\n",
    "book = gc.open_by_key(spreadsheet_key) \n",
    "worksheet = book.worksheet(wks_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'json' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-f33ef1ac9cda>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Load twitter credentials\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"covid19-sentanalysis-twitter_credentials.json\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mdatafile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m   \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdatafile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'json' is not defined"
     ]
    }
   ],
   "source": [
    "# Load twitter credentials\n",
    "with open(\"covid19-sentanalysis-twitter_credentials.json\") as datafile:\n",
    "  data = json.load(datafile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "jqdQzdqytO88",
    "outputId": "b46f1541-50e8-4fe4-a091-00175482cd65"
   },
   "outputs": [],
   "source": [
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "\n",
    "\n",
    "# Step 1 - Authenticate twitter\n",
    "consumer_key= data['consumer_key'] #'API_CONSUMER_KEY_HERE'\n",
    "consumer_secret=  data['consumer_secret']#'CONSUMER_SECRET_HERE'\n",
    "\n",
    "access_token= data['access_token_key'] #'ACCESS_TOKEN_HERE'\n",
    "access_token_secret= data['access_token_secret'] #'ACCESS_TOKEN_SECRET_HERE'\n",
    "\n",
    "\n",
    "\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# create API, set limits to avouid errors because of a timeout \n",
    "api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")\n",
    "\n",
    "#Print 5 tweets for testing purposes - Should be deleted afterwards\n",
    "home_tweets = api.home_timeline(count=5)\n",
    "print(\"printing tweets from timeline \\n \")\n",
    "for tweet in home_tweets:\n",
    "    print(tweet.text)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlNcN1cx0uOD"
   },
   "source": [
    "**GET Twitter Stream and Do Sentiment Analysis in Real time**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 70
    },
    "colab_type": "code",
    "id": "nqtpekCPtPDt",
    "outputId": "cbbfcd75-f21b-4bec-d630-5c588f420d09"
   },
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy import StreamListener\n",
    "#from google.colab import files\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "huknvBk9Y8bj"
   },
   "outputs": [],
   "source": [
    "trump = 0\n",
    "warren = 0\n",
    "\n",
    "header_name = ['Trump', 'Warren','Text','id', 'created_at', 'user_id','coordinates','location']\n",
    "with open('sentiments.csv', 'w') as file:\n",
    "  writer = csv.DictWriter(file, fieldnames = header_name)\n",
    "  writer.writeheader()\n",
    "\n",
    "# sentiments_df = pd.DataFrame(columns=['Trump','Warren'])\n",
    "#api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)\n",
    "\n",
    "class Listener(StreamListener):\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "    self.max_tweets = 10\n",
    "    self.tweet_count = 0\n",
    "    self.tweet_list = []\n",
    "    \n",
    "  def on_data(self, data):\n",
    "    raw_twitts = json.loads(data)\n",
    "    try:\n",
    "      #  Fields we need: id, created_at, text, coordinates, author_id\n",
    "      full_tweets = raw_twitts.copy()\n",
    "   \n",
    "      # TO DO: we must drop from full_tweets the tweets that consist only on RT, numbers, etc (see regex used below)\n",
    "      \n",
    "            \n",
    "      tweets = raw_twitts['text']\n",
    "      tweets = ' '.join(re.sub(\"(@[A-Za-z0-9]+) | ({*0-9A-Za-z \\t]) |] (\\wt:\\/\\/\\St+)\", \" \", tweets).split())\n",
    "      tweets = ' '.join(re.sub('RT',' ', tweets).split())  \n",
    "        \n",
    "\n",
    "      blob = TextBlob(tweets.strip())\n",
    "      global trump\n",
    "      global warren\n",
    "\n",
    "      trump_sentiment = 0\n",
    "      warren_sentiment = 0\n",
    "\n",
    "      for sent in blob.sentences:\n",
    "        if \"Trump\" in sent and \"Warren\" not in sent:\n",
    "          trump_sentiment = trump_sentiment + sent.sentiment.polarity\n",
    "        else:\n",
    "          warren_sentiment = warren_sentiment + sent.sentiment.polarity\n",
    "  \n",
    "      trump = trump + trump_sentiment\n",
    "      warren = warren + warren_sentiment\n",
    "        \n",
    "        \n",
    "      info = {'Trump': trump,'Warren': warren, 'Text':raw_twitts['text']\n",
    "               ,'id':raw_twitts['id'], 'created_at':raw_twitts['created_at']\n",
    "               , 'user_id':raw_twitts['user']['id'], 'coordinates':raw_twitts['coordinates']\n",
    "                , 'location':raw_twitts['user']['location']\n",
    "               }\n",
    "      self.tweet_list.append(info)\n",
    "    \n",
    "      print (tweets,'\\n')    \n",
    "    except:\n",
    "      print('ERROR got')\n",
    "    else:\n",
    "      self.tweet_count+=1\n",
    "    \n",
    "      # Once it reaches a fix limit the Write the data into gsheets\n",
    "      if(self.tweet_count==self.max_tweets):          \n",
    "        # save to a dataframe for eeasier file upload\n",
    "        df_tweet_list = pd.DataFrame(self.tweet_list, columns = header_name)\n",
    "        \n",
    "        d2g.upload(df_tweet_list, spreadsheet_key, wks_name, credentials=credentials, row_names=False)\n",
    "        \n",
    "        print(\"completed\")\n",
    "        return(False)\n",
    "      else:\n",
    "       decoded = json.loads(data)\n",
    "\n",
    "  def on_error(self, status):\n",
    "    print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u-MAEzOxY9k9"
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jE5iBNqMY-D5",
    "outputId": "192cc8f0-3c22-4cd0-e3c2-ea65a512c153"
   },
   "outputs": [],
   "source": [
    "twitter_stream = Stream(auth, Listener())\n",
    "twitter_stream.filter(track = ['Trump','Warren'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HTHQoydfkCz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "twitter_data_extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
