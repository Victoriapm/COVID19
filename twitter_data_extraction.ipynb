{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Covid19 - Twitter data extraction\n",
    "by Victoria, Maha, Gopi\n",
    "\n",
    "## Table of contents\n",
    "- Introduction\n",
    "- Authenticatications\n",
    "    - Twitter\n",
    "    - Google sheets\n",
    "- Gathering data & storing\n",
    "\n",
    "\n",
    "## Introduction\n",
    "This notebook is part of the project developed for the FLT Big Data Hackathon, whose objective is to create interesting and trustworthy analyses and visualizations about the COVID19 situation and its correlation with the stock market. \n",
    "\n",
    "In this notebook we use the Twitter API to retrieve the tweets related to COVID19 hashtags and economic tags, to perform a sentimental analysis and store it programatically in a google sheets file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\v.perez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Load important libraries\n",
    "import gspread \n",
    "from df2gspread import df2gspread as d2g\n",
    "from oauth2client.service_account import ServiceAccountCredentials\n",
    "import json\n",
    "import tweepy\n",
    "from textblob import TextBlob\n",
    "from tweepy import Stream\n",
    "from tweepy import StreamListener\n",
    "import pandas as pd\n",
    "import re\n",
    "import csv\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Authentication\n",
    "### Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "jqdQzdqytO88",
    "outputId": "b46f1541-50e8-4fe4-a091-00175482cd65"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Authentication OK\n",
      "printing tweets from timeline \n",
      " \n",
      "#Coronavirus Alberto Fern√°ndez anunciar√° hoy una nueva pr√≥rroga de la cuarentena \n",
      "https://t.co/5HD7s3Oeds\n",
      "\n",
      "¬°Disfruta de \"Volver\", el NUEVO texto de Paula Rom√°n,Prosa Po√©tica! ‚è¨ ‚è¨ ‚è¨  https://t.co/A4EqGFiERu\n",
      "\n",
      "Mir√° el valor que queda un iPhone XR\n",
      "\n",
      "--&gt; https://t.co/clfEwkBVJZ üì± https://t.co/9WTgWds1Kz\n",
      "\n",
      "‚Äº ESTE VIERNES ‚Äº @alarconcasanova y @odonnellmaria conversar√°n en un Instagram Live sobre Aramburu, su √∫ltimo libro‚Ä¶ https://t.co/G5aanj3jHc\n",
      "\n",
      "#Seguridad Andaba en una moto con la numeraci√≥n suprimida en plena cuarentena https://t.co/hYo5kFjN1V\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load twitter credentials\n",
    "with open(\"covid19-sentanalysis-twitter_credentials.json\") as datafile:\n",
    "  data = json.load(datafile)\n",
    "\n",
    "# Define the keys\n",
    "consumer_key= data['consumer_key'] #'API_CONSUMER_KEY_HERE'\n",
    "consumer_secret=  data['consumer_secret']#'CONSUMER_SECRET_HERE'\n",
    "\n",
    "access_token= data['access_token_key'] #'ACCESS_TOKEN_HERE'\n",
    "access_token_secret= data['access_token_secret'] #'ACCESS_TOKEN_SECRET_HERE'\n",
    "\n",
    "\n",
    "#Crate the auth object\n",
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)\n",
    "\n",
    "# create API, set limits to avouid errors because of a timeout \n",
    "api = tweepy.API(auth, wait_on_rate_limit = True, wait_on_rate_limit_notify = True)\n",
    "\n",
    "try:\n",
    "    api.verify_credentials()\n",
    "    print(\"Authentication OK\")\n",
    "except:\n",
    "    print(\"Error during authentication\")\n",
    "\n",
    "#Print 5 tweets for testing purposes - Should be deleted afterwards\n",
    "home_tweets = api.home_timeline(count=5)\n",
    "print(\"printing tweets from timeline \\n \")\n",
    "for tweet in home_tweets:\n",
    "    print(tweet.text)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Google sheets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "scope = [\n",
    "   'https://spreadsheets.google.com/feeds',\n",
    "         'https://www.googleapis.com/auth/drive']\n",
    "\n",
    "#authenticate gsheets\n",
    "google_key_file = 'service_key.json'\n",
    "credentials = ServiceAccountCredentials.from_json_keyfile_name(google_key_file, scope)\n",
    "gc = gspread.authorize(credentials)\n",
    "\n",
    "# Define spreadsheet access\n",
    "spreadsheet_key = '1auoQ9XanosnM7RUInzqeZi9EIgwtCtmtubNpXrfF6OM' \n",
    "wks_name = 'sentimentAnalysis'\n",
    "\n",
    "# Open the file\n",
    "book = gc.open_by_key(spreadsheet_key) \n",
    "worksheet = book.worksheet(wks_name) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DlNcN1cx0uOD"
   },
   "source": [
    "## Gathering data & storing\n",
    "**GET Twitter Stream and Do Sentiment Analysis in Real time**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "huknvBk9Y8bj"
   },
   "outputs": [],
   "source": [
    "trump = 0\n",
    "warren = 0\n",
    "\n",
    "header_name = ['Trump', 'Warren','Text','id', 'created_at', 'user_id','coordinates','location']\n",
    "\n",
    "class Listener(StreamListener):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.max_tweets = 10\n",
    "        self.tweet_count = 0\n",
    "        self.tweet_list = []\n",
    "    \n",
    "    def on_data(self, data):\n",
    "        raw_twitts = json.loads(data)\n",
    "        try:\n",
    "            #  Fields we need: id, created_at, text, coordinates, author_id\n",
    "            full_tweets = raw_twitts.copy()\n",
    "            # TO DO: we must drop from full_tweets the tweets that consist only on RT, numbers, etc (see regex used below)\n",
    "            tweets = raw_twitts['text']\n",
    "            tweets = ' '.join(re.sub(\"(@[A-Za-z0-9]+) | ({*0-9A-Za-z \\t]) |] (\\wt:\\/\\/\\St+)\", \" \", tweets).split())\n",
    "            tweets = ' '.join(re.sub('RT',' ', tweets).split())  \n",
    "  \n",
    "  \n",
    "            blob = TextBlob(tweets.strip())\n",
    "            global trump\n",
    "            global warren\n",
    "  \n",
    "            trump_sentiment = 0\n",
    "            warren_sentiment = 0\n",
    "  \n",
    "            for sent in blob.sentences:\n",
    "                if \"Trump\" in sent and \"Warren\" not in sent:\n",
    "                    trump_sentiment = trump_sentiment + sent.sentiment.polarity\n",
    "                else:\n",
    "                    warren_sentiment = warren_sentiment + sent.sentiment.polarity\n",
    "    \n",
    "            trump = trump + trump_sentiment\n",
    "            warren = warren + warren_sentiment\n",
    "  \n",
    "  \n",
    "            info = {'Trump': trump,'Warren': warren, 'Text':raw_twitts['text']\n",
    "                     ,'id':raw_twitts['id'], 'created_at':raw_twitts['created_at']\n",
    "                     , 'user_id':raw_twitts['user']['id'], 'coordinates':raw_twitts['coordinates']\n",
    "                      , 'location':raw_twitts['user']['location']\n",
    "                     }\n",
    "            self.tweet_list.append(info)\n",
    "  \n",
    "            print (tweets,'\\n')    \n",
    "        except:\n",
    "            print('ERROR got')\n",
    "        else:\n",
    "            self.tweet_count+=1\n",
    "                # Once it reaches a fix limit the Write the data into gsheets\n",
    "            if(self.tweet_count==self.max_tweets):          \n",
    "                # save to a dataframe for eeasier file upload\n",
    "                  df_tweet_list = pd.DataFrame(self.tweet_list, columns = header_name)\n",
    "            \n",
    "                  d2g.upload(df_tweet_list, spreadsheet_key, wks_name, credentials=credentials, row_names=False)\n",
    "            \n",
    "                  print(\"completed\")\n",
    "                  return(False)\n",
    "            else:\n",
    "                decoded = json.loads(data)\n",
    "\n",
    "        def on_error(self, status):\n",
    "            print(status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u-MAEzOxY9k9"
   },
   "outputs": [],
   "source": [
    "auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_token_secret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "jE5iBNqMY-D5",
    "outputId": "192cc8f0-3c22-4cd0-e3c2-ea65a512c153"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@poodlepingpong: Warren will destroy Pence &amp; his robotic, ignorant, evil shenanigans in one fell swoop at debates. needs to an‚Ä¶ \n",
      "\n",
      "@Gr8JohnL: Another Trump misdirection, When he harps on Mail-in voter fraud, there is less reporting on his terrible COVID-1‚Ä¶ \n",
      "\n",
      "@itsJeffTiedrich: poor Jeff Sessions can't catch a break ‚Äî too racist to be a Reagan-era judge, and not ra‚Ä¶ \n",
      "\n",
      "Sad, but true. We‚Äôre all doomed! \n",
      "\n",
      "I get what you're saying but Trump's approach doesn't engender majority favourability. It's a dangerous toolset to use. \n",
      "\n",
      "@RaheemKassam: Like this guy? https://t.co/7B7Fg2TjGx \n",
      "\n",
      "@GregMic1: Does the red wave come from the blood of nearly 100.000 Americans who died from Trump's negligence? \n",
      "\n",
      "@TheDemCoalition: ‚ÄúThere really is no record of massive fraud or even serious fraud from mail-in voting,‚Äù said the host. Tell‚Ä¶ \n",
      "\n",
      "@nprpolitics: \"If you have a problem figuring out whether you're for me or Trump, then you ain't black,\" Joe Biden said during an interv‚Ä¶ \n",
      "\n",
      "@realDonaldTrump: Sleepy Joe cannot bring us to greatness. He is the reason I‚Äôm here! https://t.co/BD21ZWVdXB \n",
      "\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "twitter_stream = Stream(auth, Listener())\n",
    "twitter_stream.filter(track = ['Trump','Warren'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7HTHQoydfkCz"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "twitter_data_extraction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
